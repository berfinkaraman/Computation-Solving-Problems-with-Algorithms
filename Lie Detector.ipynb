{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Lie Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Lie detection has been an issue since the existence of humankind. Verbal cues and physiological cues have been seen as two main ways to detect deception in a person's speech or testimony. Polygraphs are an example of a technology which tries to detect lies based on physiological cues such as blood pressure, heart rate, etc. Since polygraphs do not have the perfect streak on detecting lies, they are not widely used. Another problem with polygraphs is the fact that people can try to train their physiological responses if they have the insider information on the criteria of \"lying\" according to polygraphs. \n",
    "\n",
    "On the other hand, verbal cues can be used to detect deception. According to various research, word choice in deceptive messages varies from people's normal word choice. Since linguistic factors can indicate lies, it means that there will be a linguistic pattern that can be detected to identify deceptive messages. According to research at Cornell University and University of British Columbia, three common patterns have been seen in the deceptive speeches are:\n",
    "\n",
    "- Deceptive messages will contain more words than truthful messages. \n",
    "==> The main reason behind this phenomenon is trying to increase the engagement with the audience and make the lie more persuasive.\n",
    "- Liars will use fewer self-oriented and more other-oriented pronouns \n",
    "==> Liars tent to use \"they/their/she/he\" than \"I/my\". The psychological explanation behind this that liars tend to distance themselves from the lie.\n",
    "- Liars to use more non-committal phrases. \n",
    "==> Liars do not have to commit to a story since the story is their creation. Using non-committal words will help them to disattach it from the reality. Therefore, they tend to use non-committal words such as \"maybe,\" \"possibly,\" \"probably\" etc.\n",
    "\n",
    "I will build a lie detector that I can run my emails through so I can learn if there is a change that the other party is lying to me. If the output is positive, I can further research the situation. If it is negative, I can just trust the person who emailed me. This lie detector would be for only personal use, and it cannot be used as proof at a court.\n",
    "\n",
    "As a strategy, I can use the detected linguistic cue patterns to identify lies. However, it is unrealistic to blacklist these words because people still use words like \"they/their\" and \"probably\" in their nondeceptive text. Therefore, comparing the frequencies of these words appearing in the text can be a better way to implement a lie detector. In addition to that as I discussed linguistic cues are not the only indicator of lying. There are many other inputs that are not processed by my algorithm but still would be useful to be considered when we are questioning the truth value of a statement. Therefore, I can use my algorithm as one of the evidence of lying, but I would need more evidence to make sure the other party is lying. Also, I should not let another party to learn the logic behind my algorithm; otherwise they can train their word choice to fool the algorithm. As an improvement, I could have use machine learning to train my algorithm with word choosing style of each person individualy. Since my algorithm will be more familiar with the speaking style of each person, the error rate can be reduced. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Basically, the main idea behind my algorithm is that searching multiple patterns in a given string/text. Multiple patterns will be the words that are correlated with lying, while the given string would be the emails that I received. \n",
    "\n",
    "##### Brute Force Approach:\n",
    "Technically, I can use a brute force algorithm and try to match the array of the pattern with the parts of the main string. To make my brute force algorithm a little bit more efficient, I can try to match the first character of the pattern with the first character in the main string. If there is a match, I can keep trying to match the rest of the characters of the pattern. If any of the characters do not match, the program will keep iterating to find a match. Even though, it sounds like an easy implementation, it is not time efficient. The worst case scenario will happen if the main string and the pattern are the repetitions of the same character in different orders. The worst case time complexity would be the multiplication of the length of the main string (M) and the pattern (N) which can be expressed as O(MN) because the program will try to match each character of the pattern with each character in the main string. The following example is a good way to illustrate the worst case scenario, even though it is highly unlikely in real life.\n",
    "\n",
    "Main string: xxxxxxxxxxxo\n",
    "Len(Main string) = 12\n",
    "Pattern: xxxxxo\n",
    "Len(Pattern) = 6\n",
    "\n",
    "In this scenario, the brute force algorithm will try to match xxxxxo for each character of the main string because the first 5 character is the same. Thus for the program to find the match, it would need to iterate 12*6=72  times. Since I want my algorithm to search multiple strings (k), the worst case scenario time complexity will be O(kMN) which will not a be a good application.\n",
    "\n",
    "\n",
    "##### Rabin-Karp\n",
    "The main problem that we faced in the brute force algorithm is that we have to compare each character one by one. If we can limit the number of comparisons that the program has to perform, we can optimize our solution. We learned that hash functions could transform the input with an arbitrary size into an input with a fixed size. Having a fixed size for the pattern can increase the efficiency of our solution. If we know the hash value of the pattern, we can look for the windows/substrings in the main string that has the same hash value and the same length with the pattern. If we meet these condition, it means that it is a match. However, due to a collision, a hash function has a false positive rate which means that two different string can have the same hash value. To prevent any noise due to the false positive rate of the hash function we can perform the character by charter comparison between the pattern and the window. \n",
    "\n",
    "The hash function that we should use should be uniform. In other words, the order of the characters should matter for the hash value. Otherwise \"probably\" and \"olabpbry\" will have the same has value. This will increase the number of collisions and require more comparison than a uniform hash function.\n",
    "\n",
    "Rolling hash can be used as the hash function to compute the hash value of the window in the most optimal way. Rolling hash is a recursive hashing. Rabin Karp algorithm uses a polynomial rolling hash. Polynomial rolling hash finds the value of a string via multiplications and additions to keep track of the order of the characters while having an easy method to add a new character at the end while removing the first character of the substring.  \n",
    "\n",
    "Let a be a constant number.\n",
    "Let n be the hash values of each character.\n",
    "Let i be the index number of the character. \n",
    "Let H be the hash value of the substring.\n",
    "$$ H = n_1a^{(1-1)} + n_2a^{(2-1)} + n_3a^{(3-1)} + ... + n_ia^{(i-1)}  $$ \n",
    "\n",
    "To add a new character and remove the first character in the string we can follow the algorithm bellow:\n",
    "1. We will substract the hash value of the first character to remove its value from H. \n",
    "2. Then we will divide H with a. In that case we will have:\n",
    "\n",
    "$$ n_2a^{(2-2)} + n_3a^{(3-2)} + ... + n_ia^{(i-2)}  $$ \n",
    "\n",
    "This equation illutrates that the first value is removed and the index values of each character is got one less which means that they shift one left. \n",
    "\n",
    "3. Lastly to add the new character and relate it to its index, we will add the hash value of the new character and multiply it with \n",
    "\n",
    "$$ a^{(i-1)} $$\n",
    "\n",
    "All in all, using the rolling hash function will help us to optimize our solution by preventing the algorithm to recompute the hash value of each character each time. \n",
    "\n",
    "The worst-case scenario for Rabin-Karp is the case when all characters of the main string and the pattern are same, so the hash values of each window will match the pattern. In that case, it will become the almost same as the brute force algorithm that we discussed above because the program has to compare each string one by one. The worst case time complexity is O((M-(N+1))N) becuase the number of comaprision that will be made still be less than trying each value since the progra compares based on the window size. Thus, Rabin-Karp will be more efficient because it is worst case scenario is less likely and a little bit more efficient compared to the brute force algorithm. Also, the average and best case time complexity of Rabin-Karp is O(M+N) which is a more common scenarios than the worst case. M comes from the number of hash value comparison and in addition that N is the time complexity for searching the window. If we are looking for k pattern in one list the time compleity would be O(k(M+N)). \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage frequency of first person pronouns appearing in the email is 8.24742268041 %\n",
      "The percentage frequency of third person pronouns appearing in the email is 1.03092783505 %\n",
      "The percentage frequency of non-commital words appearing in the email is 0.0 %\n",
      "The word count of the email is 97\n",
      "The email is telling the/a Truth\n"
     ]
    }
   ],
   "source": [
    "def RabinKarp(pattern, text, a): \n",
    "    #Firstly I will define the length of the pattern to find the length of the window\n",
    "    N = len(pattern) \n",
    "    #Then I will define the length of the main string to know the upper bound\n",
    "    M = len(text)\n",
    "    #n is the hash value of the pattern\n",
    "    n = 0 \n",
    "    #t is the hash value for the window in the main text\n",
    "    t = 0  \n",
    "    h=1\n",
    "    #I will asume that I will only recieve emails in latin alphabeth thus the number of chracters will be 256\n",
    "    d = 256\n",
    "    #counter to count how many time pattern appears in the text\n",
    "    count = 0\n",
    "    \n",
    "    for i in xrange(N-1): \n",
    "        h = (h*d)%a\n",
    "        \n",
    "    #For loop to process the hash value of the pattern and the first window in the text\n",
    "    for i in range(N): \n",
    "        #hash value of pattern\n",
    "        n = (d*n + ord(pattern[i]))%a \n",
    "        #hash value of the first window\n",
    "        t = (d*t + ord(text[i]))%a\n",
    "  \n",
    "    #For loop to keep checking the hash value of the pattern with different windows\n",
    "    for i in range(M-N+1): \n",
    "        #starting from the first window check if the hash value of the pattern equals to the hash value of the window\n",
    "        if n==t: \n",
    "            #assining a match value to use it for count\n",
    "            match = True\n",
    "            #then compare the each character one by one to make sure the match is not due to false positive  \n",
    "            for j in range(N): \n",
    "                if text[i+j] != pattern[j]:\n",
    "                    match = False\n",
    "                    break\n",
    "            #if the hash values are equal and the charcter by charcter check didn't break, it means that the pattern exist in the main text\n",
    "            if match: \n",
    "                count += 1\n",
    "  \n",
    "        #If loop to check if we are still in the frame and iniate the calculation of the next window\n",
    "        if i < M-N:\n",
    "            #As we recall from the rolling hash algorithm firstly remove the first character which is (t-ord(text[i])*h) part of the code\n",
    "            #Then we add the new value to the end which is  ord(text[i+N]) part of the code\n",
    "            t = (d*(t-ord(text[i])*h) + ord(text[i+N]))%a \n",
    "            # Checks if we have a negative value of the hash value of the window \n",
    "            if t < 0: \n",
    "                #assigns a positive value to the hash value of the window\n",
    "                t = (t+a)%a\n",
    "    #returns the number of time the pattern appears in the main text\n",
    "    return count\n",
    "  \n",
    "#Test data\n",
    "\n",
    "\n",
    "#Different category patterns that are associated with lying\n",
    "#I left a gap befor and after each word to prevent the algorithm find them within other words\n",
    "personal_pronoun= [\" I \",\" I'm \",\" I've \",\" I'd \",\" I'll \",\" me \",\" my \",\" mine \"]\n",
    "third_person= [\" he \",\" she \",\" they \",\" their \",\" them \",\" her \",\" his \",\" him \", \" it \",\" he's \",\" she's \",\" they've \", \" they're \"]\n",
    "non_commital = [\" probably \", \" maybe \", \" might \", \" may \", \" possibly \",\" likely \"]\n",
    "\n",
    "\n",
    "email = \"Hey Berfin, I read on The Morning that Bridge is opened for submission but don't know what topic you are looking for. Do you prefer black and white photos over color ones? And how many photos can I submit? I'm in SF this semester because I broke my ankle 2 days before flying to Seoul :( Nevertheless, I want to stay in touch with everyone in Seoul and I think the Zine is an excellent way to do that. Thank you for keeping it alive! Have a great week and I look forward to your response. Best,\"\n",
    "\n",
    "#I will define word count algorithm to find the word count of the main text to find the frequencies\n",
    "def word_count (text):\n",
    "    #splits text according to word gaps\n",
    "    words = text.split(\" \")\n",
    "    #returns the length of the words list which has each word in the text\n",
    "    return len(words)\n",
    "\n",
    "#I will define the constant as 103 because it is big enough prime number smaller than 256\n",
    "a = 103 \n",
    "\n",
    "#I will define a function to find the frequencies of each category\n",
    "def perc_category (ctg, text):\n",
    "    #gets the word count of text\n",
    "    w = word_count(text)\n",
    "    #parameter for frequence of the lie words\n",
    "    freq = 0\n",
    "    #runs the Rabin-Karp algorithm for each pattern in each category\n",
    "    for i in ctg:\n",
    "        #sums up the frequences\n",
    "        freq += RabinKarp(i,text,a)\n",
    "    #finds the percentage of a category of pattern apperance in the main text\n",
    "    perc = float(freq/w)*100\n",
    "    return perc \n",
    "\n",
    "print \"The percentage frequency of first person pronouns appearing in the email is\",perc_category (personal_pronoun, email),\"%\"\n",
    "print \"The percentage frequency of third person pronouns appearing in the email is\",perc_category (third_person, email),\"%\"\n",
    "print \"The percentage frequency of non-commital words appearing in the email is\",perc_category (non_commital, email),\"%\"\n",
    "print \"The word count of the email is\",word_count(email)\n",
    "\n",
    "\n",
    "#I came up with the following criteria based on my background reseach in the field. \n",
    "#1st condition: When people tell a lie they tend to use more third person pronouns (Gupta, 2007)\n",
    "#2nd condition: Average email size is 200. Since liars tend to use more word, I expect lies to be longer than 200 words. (Garber, 2013)\n",
    "#3rd condition: The percentage of average non-commital word usage by liars is 0.7 (including the standart error) (Dou, Liu, Muneer,& Schlussel, 2016)\n",
    "def truth_or_lie(text):\n",
    "    if perc_category (personal_pronoun, text) < perc_category (third_person, text) and word_count(text) > 200 and perc_category (non_commital, text) > 0.7:\n",
    "        l = \"Lie\"\n",
    "        return l\n",
    "    else:\n",
    "        t = \"Truth\"\n",
    "        return t\n",
    "    \n",
    "print \"The email is telling the/a\", truth_or_lie(email)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Conclusion\n",
    "\n",
    "After I run my email on my program, I conclude that the email that I received didn't have enough lie indicator. Since I continue this conversation in real life, I know that the person wasn't lying. However, I need more test to establish the success of my program. As I discussed before this program cannot be used for an absolute conclusion whether a statement is a lie or truth. In addition to that this program evaluates the overall text thus it may overlook small lies or a sentence that was a lie. \n",
    "\n",
    "However, as I was creating this program, I realized other possible applications of the program. The string searching can be used for a spam detector. If you identify spam email related words such as \"Free Coupon,\" \"Congratulations\", etc., you can assign a spam value to each email and avoid reading unnecessary email. Another application can be a search engine for you to find the required research paper for your assignment. You can enter keywords that you are looking for and run various resources through this program to asses how related the source that you found. If it passes specific criteria, you can spend more time on deep reading. The last version partially sounds like any search engine, but without the biases, your searching engine has about you. \n",
    "\n",
    "All in all, this program can be improved via machine learning to give more accurate results or bloom filter to store more data for comparison. With the further improvements, it can turn into something people can use on a daily basis to ease some of their tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Resources:\n",
    "\n",
    "Cormen et al., ibid, Part IV, section 23.2\n",
    "\n",
    "\n",
    "Dou, J., Liu, M., Muneer, H., & Schlussel, A. (2016). What Words Do We Use to Lie?: Word Choice in Deceptive Messages. Cornell University CALS. Retrieved December 20, 2018, from https://arxiv.org/ftp/arxiv/papers/1710/1710.00273.pdf.\n",
    "\n",
    "\n",
    "Garber, M. (2013, January 08). You Probably Write a Novel's Worth of Email Every Year. Retrieved December 20, 2018, from https://www.theatlantic.com/technology/archive/2013/01/you-probably-write-a-novels-worth-of-email-every-year/266942/\n",
    "\n",
    "\n",
    "Gupta, S. (2007). Modelling Deception Detection in Text. Queenâ€™s University School of Computing. Retrieved December 20, 2018, from https://www.collectionscanada.gc.ca/obj/s4/f2/dsk3/OKQ/TC-OKQ-922.pdf.\n",
    "\n",
    "\n",
    "J, Hancock. L, Curry. S, Goorha., & M, Woodworth. (2008). On Lying and Being Lied To: A Linguistic Analysis of Deception in Computer-Mediated Communication. Discourse Processes. 45. 1-23.  Retrieved December 20, 2018, from DOI: 10.1080/01638530701739181. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Appendix:\n",
    "##### #algorithms: \n",
    "While I was discussing how to implement the polynomial rolling hash function, I come up with steps to follow to shift the window. I identified three steps to be followed to shift the window one character left. Then I used this algorithm to implement the rolling hash function for my program.\n",
    "##### #strategize:\n",
    "I defined my idea and intention clearly. Then I defined what are the weaknesses and strength of my idea. I improved some of the weaknesses of my program. However, in the end, I acknowledge the weaknesses of my program and gave possible improvements to be made.\n",
    "##### #breakitdown: \n",
    "Initially, I had a problem in hand to solve (if a person is lying or not). Then I discussed possible ways of lie detection. Then break my question into components to be tackled. When I was implementing the program, firstly, I tried to code the Rabin-Karp than added other codes to utilize the Rabin-Karp algorithm for my purpose. Break it down is an important HC when you are trying to build a program try to identify which subproblems you should tackle to reach the conclusion. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SageMath (stable)",
   "language": "sagemath",
   "name": "sagemath"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}